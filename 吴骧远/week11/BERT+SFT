# coding:utf8

import torch
import torch.nn as nn
import numpy as np
import math
import random
import os
import re
import json

"""
基于pytorch的BERT+mask自回归语言模型 - SFT训练（内容生成标题）
"""

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class LanguageModel(nn.Module):
    def __init__(self, input_dim, vocab):
        super(LanguageModel, self).__init__()
        self.input_dim = input_dim
        self.vocab_size = len(vocab)
        self.vocab = vocab  # 保存词表引用

        self.embedding = nn.Embedding(len(vocab), input_dim)
        self.pos_encoding = PositionalEncoding(input_dim)

        # 使用Transformer编码器层
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_dim,
            nhead=8,  # 多头注意力头数
            dim_feedforward=input_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)

        self.classify = nn.Linear(input_dim, len(vocab))
        self.dropout = nn.Dropout(0.1)
        self.loss = nn.functional.cross_entropy

    def create_sft_mask(self, input_ids, sep_token_id):
        """创建SFT掩码：S1部分（内容）相互可见，S2部分（标题）只能看到左边上下文"""
        batch_size, seq_len = input_ids.shape
        mask = torch.full((seq_len, seq_len), float('-inf'))

        for b in range(batch_size):
            # 找到分隔符位置
            sep_positions = (input_ids[b] == sep_token_id).nonzero(as_tuple=True)[0]
            if len(sep_positions) > 0:
                sep_pos = sep_positions[0].item()
            else:
                sep_pos = seq_len  # 如果没有分隔符，全部当作S1处理

            # S1部分（内容）：相互可见
            if sep_pos > 0:
                mask[:sep_pos, :sep_pos] = 0

            # S2部分（标题）：只能看到左边的上下文
            for i in range(sep_pos, seq_len):
                mask[i, :i + 1] = 0

        return mask

    def forward(self, x, y=None, loss_mask=None):
        batch_size, seq_len = x.shape
        input_ids = x  # 保存原始输入ID用于创建掩码

        # 词嵌入和位置编码
        x = self.embedding(x) * math.sqrt(self.input_dim)  # 缩放嵌入
        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)  # 位置编码

        # 创建SFT掩码
        sft_mask = self.create_sft_mask(input_ids, self.vocab.get('<sep>', 1))
        if torch.cuda.is_available():
            sft_mask = sft_mask.cuda()

        # Transformer编码
        x = self.transformer(x, mask=sft_mask)

        # 分类层
        y_pred = self.classify(x)  # output shape:(batch_size, seq_len, vocab_size)

        if y is not None:
            # 只对标题部分计算loss
            if loss_mask is not None:
                # 使用loss_mask只计算标题部分的损失
                loss = self.loss(y_pred.view(-1, y_pred.shape[-1]), y.view(-1), reduction='none')
                loss = loss * loss_mask.view(-1)  # 应用mask
                mask_sum = loss_mask.sum()
                if mask_sum > 0:
                    return loss.sum() / mask_sum  # 平均损失
                else:
                    return torch.tensor(0.0, device=loss.device, requires_grad=True)
            else:
                return self.loss(y_pred.view(-1, y_pred.shape[-1]), y.view(-1))
        else:
            return torch.softmax(y_pred, dim=-1)


# 加载字表
def build_vocab(vocab_path):
    vocab = {"<pad>": 0, "<sep>": 1, "<eos>": 2}  # 添加特殊token
    with open(vocab_path, encoding="utf8") as f:
        for index, line in enumerate(f):
            char = line[:-1]  # 去掉结尾换行符
            vocab[char] = index + 3  # 留出位置给特殊token
    vocab["<UNK>"] = len(vocab)  # 未知token
    return vocab


# 加载新闻数据
def load_news_data(data_path):
    """从JSON格式的新闻数据中加载数据"""
    news_data = []
    with open(data_path, encoding="utf8") as f:
        for line in f:
            if line.strip():
                try:
                    data = json.loads(line.strip())
                    # 构建SFT格式：内容<sep>标题<eos>（内容生成标题）
                    sft_text = data["content"] + "<sep>" + data["title"] + "<eos>"
                    news_data.append(sft_text)
                except:
                    continue
    return news_data


# 构建SFT样本
def build_sft_sample(vocab, window_size, text):

    if len(text) < window_size:
        # 如果文本太短，进行padding
        text = text + "<pad>" * (window_size - len(text))
    elif len(text) > window_size:
        # 如果文本太长，进行截断
        text = text[:window_size]

    # 输入和目标错开一位
    input_text = text[:-1]
    target_text = text[1:]

    # 转换为ID
    x = [vocab.get(char, vocab["<UNK>"]) for char in input_text]
    y = [vocab.get(char, vocab["<UNK>"]) for char in target_text]

    # 创建损失掩码：只对标题部分计算loss
    loss_mask = []
    sep_found = False

    # 找到<sep>在原文本中的位置
    sep_pos_in_original = text.find('<sep>')

    for i, char in enumerate(target_text):
        # 对应原文本中的位置是i+1 (因为target_text是从位置1开始的)
        original_pos = i + 1

        if original_pos <= sep_pos_in_original:
            # 内容部分不参与loss计算
            loss_mask.append(0.0)
        else:
            # 标题部分参与loss计算 (包括<sep>之后的所有内容)
            loss_mask.append(1.0)

    # 补齐loss_mask长度
    while len(loss_mask) < len(y):
        loss_mask.append(0.0)

    return x, y, loss_mask


# 建立SFT数据集
def build_sft_dataset(news_data, vocab, window_size, sample_length):
    """构建SFT数据集"""
    dataset_x = []
    dataset_y = []
    dataset_loss_mask = []

    for i in range(sample_length):
        # 随机选择一条新闻
        text = random.choice(news_data)
        x, y, loss_mask = build_sft_sample(vocab, window_size, text)
        dataset_x.append(x)
        dataset_y.append(y)
        dataset_loss_mask.append(loss_mask)

    return torch.LongTensor(dataset_x), torch.LongTensor(dataset_y), torch.FloatTensor(dataset_loss_mask)

# 建立模型
def build_model(vocab, char_dim):
    model = LanguageModel(char_dim, vocab)
    return model


# 文本生成测试代码
def generate_title(content, model, vocab, window_size, max_length=30):
    """根据内容生成标题"""
    reverse_vocab = dict((y, x) for x, y in vocab.items())
    model.eval()
    with torch.no_grad():
        # 构建输入：内容<sep>
        input_text = content + "<sep>"
        generated_title = ""

        for _ in range(max_length):
            # 准备输入序列
            current_input = input_text + generated_title
            if len(current_input) > window_size:
                current_input = current_input[-window_size:]

            x = [vocab.get(char, vocab["<UNK>"]) for char in current_input]
            x = torch.LongTensor([x])
            if torch.cuda.is_available():
                x = x.cuda()

            # 预测下一个字符
            y = model(x)[0][-1]
            index = sampling_strategy(y)
            pred_char = reverse_vocab[index]

            # 如果生成结束符，停止生成
            if pred_char in ["<eos>", "<pad>"]:
                break

            generated_title += pred_char

    return generated_title

def sampling_strategy(prob_distribution, temperature=1.0, top_k=50, top_p=0.9):

    if torch.isnan(prob_distribution).any() or torch.isinf(prob_distribution).any():
        prob_distribution = torch.ones_like(prob_distribution)

    prob_distribution = prob_distribution / temperature
    prob_distribution = torch.softmax(prob_distribution, dim=-1)

    if torch.isnan(prob_distribution).any():
        prob_distribution = torch.ones_like(prob_distribution) / prob_distribution.size(-1)

    # Top-k采样：只考虑概率最高的k个token
    if top_k > 0:
        top_k_probs, top_k_indices = torch.topk(prob_distribution, min(top_k, prob_distribution.size(-1)))
        # 创建mask，只保留top-k的概率
        prob_distribution = torch.zeros_like(prob_distribution)
        prob_distribution.scatter_(-1, top_k_indices, top_k_probs)

    # Top-p采样：只考虑累积概率达到p的token
    if top_p < 1.0:
        sorted_probs, sorted_indices = torch.sort(prob_distribution, descending=True)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

        # 找到累积概率超过top_p的位置
        sorted_indices_to_remove = cumulative_probs > top_p
        # 保留第一个超过的token
        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
        sorted_indices_to_remove[0] = False

        # 将不需要的token概率设为0
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        prob_distribution[indices_to_remove] = 0

    # 归一化
    prob_sum = prob_distribution.sum()
    if prob_sum > 0:
        prob_distribution = prob_distribution / prob_sum
    else:
        # 如果所有概率都为0，使用均匀分布
        prob_distribution = torch.ones_like(prob_distribution) / prob_distribution.size(-1)

    # 转换为numpy并进行最终检查
    prob_distribution = prob_distribution.cpu().numpy()

    if np.isnan(prob_distribution).any() or (prob_distribution < 0).any():
        prob_distribution = np.ones_like(prob_distribution) / len(prob_distribution)

    # 确保概率和为1
    if prob_distribution.sum() == 0:
        prob_distribution = np.ones_like(prob_distribution) / len(prob_distribution)
    else:
        prob_distribution = prob_distribution / prob_distribution.sum()

    return np.random.choice(list(range(len(prob_distribution))), p=prob_distribution)


# 计算文本ppl
def calc_perplexity(sentence, model, vocab, window_size):
    prob = 0
    model.eval()
    with torch.no_grad():
        for i in range(1, len(sentence)):
            start = max(0, i - window_size)
            window = sentence[start:i]
            x = [vocab.get(char, vocab["<UNK>"]) for char in window]
            x = torch.LongTensor([x])
            target = sentence[i]
            target_index = vocab.get(target, vocab["<UNK>"])
            if torch.cuda.is_available():
                x = x.cuda()
            pred_prob_distribute = model(x)[0][-1]
            target_prob = pred_prob_distribute[target_index]
            prob += math.log(target_prob, 10)
    return 2 ** (prob * (-1 / len(sentence)))

def train_sft(news_data_path, vocab_path, save_weight=True):
    """SFT训练主函数"""
    epoch_num = 10  # 减少训练轮数
    batch_size = 16  # 减小batch size
    train_sample = 1000  # 减少每轮训练样本数
    char_dim = 256  # 减小模型维度
    window_size = 80  # 适当增加窗口大小以容纳内容+标题

    # 构建词表
    vocab = build_vocab(vocab_path)
    print(f"词表大小: {len(vocab)}")

    # 加载新闻数据
    news_data = load_news_data(news_data_path)
    print(f"加载新闻数据: {len(news_data)}条")

    # 建立模型
    model = build_model(vocab, char_dim)
    if torch.cuda.is_available():
        model = model.cuda()

    optim = torch.optim.Adam(model.parameters(), lr=0.0001)  # 降低学习率防止梯度爆炸
    print("SFT语言模型加载完毕，开始训练（内容生成标题）")

    for epoch in range(epoch_num):
        model.train()
        watch_loss = []
        for batch in range(int(train_sample / batch_size)):
            x, y, loss_mask = build_sft_dataset(news_data, vocab, window_size, batch_size)
            if torch.cuda.is_available():
                x, y, loss_mask = x.cuda(), y.cuda(), loss_mask.cuda()
            optim.zero_grad()  # 梯度归零
            loss = model(x, y, loss_mask)  # 计算loss，只对标题部分

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"警告：第{epoch + 1}轮第{batch + 1}批次出现NaN/Inf损失，跳过此批次")
                continue

            loss.backward()  # 计算梯度

            # 梯度裁剪防止梯度爆炸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optim.step()  # 更新权重
            watch_loss.append(loss.item())

        print("=========\n第%d轮平均loss:%f" % (epoch + 1, np.mean(watch_loss)))

        # 测试生成效果
        print("生成测试:")
        test_content1 = "阿根廷布宜诺斯艾利斯省奇尔梅斯市一服装店，8个月内被抢了三次。"
        print(f"内容：{test_content1}")
        print(f"生成标题：{generate_title(test_content1, model, vocab, window_size)}")

        test_content2 = "北京全市范围公共场所、工作场所室内环境及公共交通工具内将禁止吸烟"
        print(f"内容：{test_content2}")
        print(f"生成标题：{generate_title(test_content2, model, vocab, window_size)}")
        print()

    if save_weight:
        model_path = "sft_title_generation_model.pth"
        torch.save(model.state_dict(), model_path)
        print(f"模型已保存到: {model_path}")


if __name__ == "__main__":
    if not os.path.exists("vocab.txt"):
        print("创建词表文件...")
        # 从新闻数据中提取字符创建词表
        chars = set()
        with open("sample_data.json", encoding="utf8") as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line.strip())
                        text = data["title"] + data["content"]
                        chars.update(text)
                    except:
                        continue

        with open("vocab.txt", "w", encoding="utf8") as f:
            for char in sorted(chars):
                f.write(char + "\n")
        print(f"词表创建完成，包含{len(chars)}个字符")

    # 开始SFT训练
    train_sft("sample_data.json", "vocab.txt", True)
