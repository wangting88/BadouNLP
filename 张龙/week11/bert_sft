import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import BertModel, BertTokenizer
import json

class BertSFT(nn.Module):
    def __init__(self, model_name, vocab_size):
        super(BertSFT, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.classify = nn.Linear(self.bert.config.hidden_size, vocab_size)
        self.loss = nn.CrossEntropyLoss(ignore_index=-1)

    def forward(self, x, mask=None, y=None):
        if y is not None:
            # 训练阶段
            x = self.bert(x, attention_mask=mask)[0]  # 取 last_hidden_state
            y_pred = self.classify(x)  # (batch_size, seq_len, vocab_size)
            return self.loss(y_pred.view(-1, y_pred.shape[-1]), y.view(-1))
        else:
            # 推理阶段
            x = self.bert(x)[0]
            y_pred = self.classify(x)
            return torch.softmax(y_pred, dim=-1)

def create_mask(s1_len, s2_len):
    len_s1 = s1_len + 2  # [CLS] and [SEP]
    len_s2 = s2_len + 1  # [SEP]
    total_len = len_s1 + len_s2
    mask = torch.ones((total_len, total_len), dtype=torch.long)

    # prompt（s1）不能看 answer（s2）
    for i in range(len_s1):
        mask[i, len_s1:] = 0

    # answer（s2）不能看未来的 answer token
    for i in range(len_s2):
        mask[len_s1 + i, len_s1 + i + 1:] = 0

    return mask

def pad_mask(mask, size):
    padded_mask = torch.zeros(size, dtype=torch.long)
    padded_mask[:mask.size(0), :mask.size(1)] = mask
    return padded_mask

def build_dataloader(file_path, tokenizer, batch_size=8, max_length=128):
    dataset = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            prompt = data['content']
            answer = data['title']

            prompt_encode = tokenizer.encode(prompt, add_special_tokens=False)
            answer_encode = tokenizer.encode(answer, add_special_tokens=False)

            # 构建输入：[CLS] prompt [SEP] answer [SEP]
            x = [tokenizer.cls_token_id] + prompt_encode + [tokenizer.sep_token_id] + answer_encode + [tokenizer.sep_token_id]

            # 构建标签：前为 -1（不算 loss），后为 answer + [SEP]
            y = [-1] * (len(prompt_encode) + 2) + answer_encode + [tokenizer.sep_token_id]

            # 构建 mask
            mask = create_mask(len(prompt_encode), len(answer_encode))
            mask = mask[:max_length, :max_length]  # 裁剪为 max_length
            mask = pad_mask(mask, (max_length, max_length))  # 可选，已一致时无效影响

            # padding
            x = x[:max_length] + [0] * (max_length - len(x))
            y = y[:max_length] + [-1] * (max_length - len(y))

            dataset.append([torch.LongTensor(x), mask, torch.LongTensor(y)])

    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

def train(model, dataloader, optimizer, device, epochs=3):
    model.train()
    model.to(device)
    for epoch in range(epochs):
        total_loss = 0.0
        for batch in dataloader:
            x, mask, y = [b.to(device) for b in batch]
            loss = model(x, mask, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}")

def main():
    model_name = r"D:\nlp\第六周 语言模型\bert-base-chinese\bert-base-chinese"
    tokenizer = BertTokenizer.from_pretrained(model_name)

    data_path = r"D:\nlp\第十周\week10 文本生成问题\transformers-生成文章标题\sample_data.json"

    dataloader = build_dataloader(data_path, tokenizer, batch_size=8, max_length=128)

    model = BertSFT(model_name=model_name, vocab_size=tokenizer.vocab_size)

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    train(model, dataloader, optimizer, device, epochs=3)

    torch.save(model.state_dict(), "bert_sft.pth")
    print("✅ 模型已保存为 bert_sft.pth")

if __name__ == "__main__":
    main()
